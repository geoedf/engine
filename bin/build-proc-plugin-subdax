#!/usr/bin/env python2

# builds a Pegasus subdax for a processor plugin and saves to an XML file

import sys
import os
import yaml
from yaml import FullLoader
import json

from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization

from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import padding

#sys.path.append('/usr/lib/python2.7/dist-packages')

from Pegasus.DAX3 import *

from geoedfengine.helper.GeoEDFProcessor import GeoEDFProcessor
from geoedfengine.helper.WorkflowUtils import WorkflowUtils

# process command line arguments:
# workflow filepath
# workflow stage
# plugin name to be used to identify right executable (from right container)
# subdax filepath
# remote job directory (prefix)
# run directory
# stage references as comma separated string
# arg to local file bindings encoded as JSON string
# sensitive arg bindings
# target site

# basic validation on number of args
if len(sys.argv) < 9:
    raise Exception("Insufficient arguments to processor plugin subdax construction job")

# extract the args
workflow_filename = str(sys.argv[1])
workflow_stage = str(sys.argv[2])
plugin_name = str(sys.argv[3])
subdax_filename = str(sys.argv[4])
job_dir = str(sys.argv[5])
run_dir = str(sys.argv[6])
stage_refs_str = str(sys.argv[7])
local_file_binds_str = str(sys.argv[8])
sensitive_arg_binds_str = str(sys.argv[9])
target = str(sys.argv[10])

# initialize DAX, set up some basic executables
dax = ADAG("stage-%s-Processor" % workflow_stage)

# add the workflow file to this DAX
workflow_fname = os.path.split(workflow_filename)[1]
workflow_dax_file = File(workflow_fname)
workflow_dax_file.addPFN(PFN("file://%s" % workflow_filename,"local"))
dax.addFile(workflow_dax_file)

# get a helper
helper = WorkflowUtils()

# process the local file arg bindings JSON
if local_file_binds_str != 'None':
    local_file_args_exist = True
    local_file_binds = json.loads(local_file_binds_str)
    
    # create comma separated string of local file args and list of filepath vals
    local_file_args = list(local_file_binds.keys())

    local_files_needed = []
    for arg in local_file_args:
        local_files_needed.append(local_file_binds[arg])

    local_file_args_str = '%s' % local_file_args[0]

    for arg in local_file_args[1:]:
        local_file_args_str = '%s,%s' % (local_file_args_str,arg)
else:
    local_file_args_exist = False
    local_file_args_str = 'None'

# if local file args exist, add input files to DAX
local_dax_files = []
other_local_dax_files = []
if local_file_args_exist:
    for local_file in local_files_needed:
        local_file_fname = os.path.split(local_file)[1]
        local_dax_file = File(local_file_fname)
        local_dax_file.addPFN(PFN("file://%s" % local_file,"local"))
        dax.addFile(local_dax_file)
        local_dax_files.append(local_dax_file)
        # special handling for shapefiles; also add other related files (dbf,shx,prj, etc.)
        # check if shapefile
        local_file_ext = os.path.splitext(local_file_fname)[1]
        if local_file_ext == '.shp':
            # find all other files with same basename
            basename = os.path.splitext(local_file_fname)[0]
            # assumes fully qualified path provided
            local_file_dir = os.path.split(local_file)[0]
            for local_dir_file in os.listdir(local_file_dir):
                full_path = '%s/%s' % (local_file_dir,local_dir_file)
                if os.path.isfile(full_path):
                    local_dir_file_ext = os.path.splitext(local_dir_file)[1]
                    if local_dir_file.startswith(basename) and local_dir_file_ext != '.shp':
                        # add these also to DAX as inputs
                        other_local_dax_file = File(local_dir_file)
                        other_local_dax_file.addPFN(PFN("file://%s" % full_path, "local"))
                        dax.addFile(other_local_dax_file)
                        other_local_dax_files.append(other_local_dax_file)

# process sensitive arg bindings if any, encrypting them with the public key
# create a new JSON dictionary with arg to encrypted value pairs
if sensitive_arg_binds_str != 'None':
    sensitive_arg_binds = json.loads(sensitive_arg_binds_str)
    encrypted_arg_binds = dict()

    # process these bindings, encrypting their value
    workflow_pubkey_filename = '%s/output/public.pem' % run_dir

    with open(workflow_pubkey_filename,'rb') as key_file:
        public_key = serialization.load_pem_public_key(
            key_file.read(),
            backend=default_backend())

        for arg in sensitive_arg_binds:
            sensitive_val = sensitive_arg_binds[arg]
            sensitive_val_bytes = bytes(sensitive_val,'utf-8')
            encrypted_val = public_key.encrypt(
                sensitive_val_bytes,
                padding.OAEP(
                    mgf=padding.MGF1(algorithm=hashes.SHA256()),
                    algorithm=hashes.SHA256(),
                    label=None))

            #base64 encode val so it can be transmitted via JSON
            b64encoded_val = base64.encodestring(encrypted_val)
            
            encrypted_arg_binds[arg] = b64encoded_val.decode('ascii')
            
    encrypted_arg_binds_str = json.dumps(json.dumps(encrypted_arg_binds))

else:
    encrypted_arg_binds_str = 'None'
    

# extract the workflow stage
with open(workflow_filename,'r') as workflow_file:
    try:
        workflow_dict = yaml.load(workflow_file,Loader=FullLoader)

        stage_id = '$%s' % workflow_stage

        # process stage references for this plugin (processors cannot have vars)
        # reconvert back to list
        # determine output files holding their values
        # and create a dictionary of stage to value bindings
        stage_refs_exist = False
        
        if stage_refs_str != 'None':
            stage_refs_exist = True
            stage_refs = stage_refs_str.split(',')
            stage_ref_values = dict()
            for stage_ref in stage_refs:
                stage_ref_values[stage_ref] = []
                stage_ref_val_filename = '%s/outputs/results_%s.txt' % (run_dir,stage_ref)
                with open (stage_ref_val_filename,'r') as stage_ref_val_file:
                    for val in stage_ref_val_file:
                        stage_ref_values[stage_ref].append(val.rstrip())

        # if stage refs exist, build binding combinations and corresponding parallel jobs
        if stage_refs_exist:

            binding_combs = helper.create_binding_combs(stage_ref_values,None)

            indx = 0
            plugin_jobs = []
            res_files = []
            for binding in binding_combs:

                stage_binds_str = json.dumps(json.dumps(binding))

                # create job for this plugin
                # executable name is different for each proc since
                # it needs to be run in the processor's own container
                exec_name = "run-processor-%s" % plugin_name.lower()
                plugin_job = Job(name=exec_name)

                # args:
                # workflow_file
                # workflow_stage
                # plugin type
                # output_path
                # stage refs JSON str
                # encrypted arg binds str
                # local file args str
                # optional trailing list of file args (local files converted to inputs)
                plugin_job.addArguments(workflow_dax_file)
                plugin_job.uses(workflow_dax_file, link=Link.INPUT)
                
                plugin_job.addArguments(workflow_stage)
            
                output_dir = '%s/%s' % (job_dir,workflow_stage)
                plugin_job.addArguments("Processor")
                plugin_job.addArguments(output_dir)
                
                plugin_job.addArguments(stage_binds_str)
                plugin_job.addArguments(encrypted_arg_binds_str)
                plugin_job.addArguments(local_file_args_str)

                for local_dax_file in local_dax_files:
                    plugin_job.addArguments(local_dax_file)
                    plugin_job.uses(local_dax_file, link=Link.INPUT)
                    
                for other_local_dax_file in other_local_dax_files:
                    plugin_job.uses(other_local_dax_file, link=Link.INPUT)
                    
                dax.addJob(plugin_job)
                plugin_jobs.append(plugin_job)

                indx += 1

        else: # no bindings
            plugin_jobs = []
            res_files = []
            
            stage_binds_str = 'None'

            # create job for this plugin
            # executable name is different for each proc since
            # it needs to be run in the processor's own container
            exec_name = "run-processor-%s" % plugin_name.lower()
            plugin_job = Job(name=exec_name)

            # args:
            # workflow_file
            # workflow_stage
            # plugin type
            # output_path
            # stage refs JSON str
            # encrypted arg binds str
            # local file args str
            # optional trailing list of file args (local files converted to inputs)
            plugin_job.addArguments(workflow_dax_file)
            plugin_job.uses(workflow_dax_file, link=Link.INPUT)
                
            plugin_job.addArguments(workflow_stage)
            
            output_dir = '%s/%s' % (job_dir,workflow_stage)
            plugin_job.addArguments("Processor")
            plugin_job.addArguments(output_dir)
                
            plugin_job.addArguments(stage_binds_str)
            plugin_job.addArguments(encrypted_arg_binds_str)
            plugin_job.addArguments(local_file_args_str)

            for local_dax_file in local_dax_files:
                plugin_job.addArguments(local_dax_file)
                plugin_job.uses(local_dax_file, link=Link.INPUT)
                    
            for other_local_dax_file in other_local_dax_files:
                plugin_job.uses(other_local_dax_file, link=Link.INPUT)
                    
            dax.addJob(plugin_job)
            plugin_jobs.append(plugin_job)

        # collect output file names
        collect_job = Job(name="collect.py")
        output_dir = '%s/%s' % (job_dir,workflow_stage)
        collect_job.addArguments(workflow_stage)
        collect_job.addArguments(output_dir)
        collect_res_filename = 'results_%s.txt' % workflow_stage
        collect_res_file = File(collect_res_filename)
        dax.addFile(collect_res_file)
        collect_job.uses(collect_res_file, link=Link.OUTPUT)
        dax.addJob(collect_job)
        for plugin_job in plugin_jobs:
            dax.depends(parent=plugin_job,child=collect_job)

        # write out to DAX xml file
        with open(subdax_filename,'w') as subdax_file:
            dax.writeXML(subdax_file)
    except:
        raise

